<!DOCTYPE html>
<!-- saved from url=(0034)https://raymin0223.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Sangmin Bae</title>
<meta content="Sangmin Bae" name="Sangmin Bae">
<link href="./Sangmin_Bae_files/style.css" rel="stylesheet" type="text/css">
<script src="./Sangmin_Bae_files/jquery-1.11.1.min.js" type="text/javascript"></script>  
</head>


<body>
  <div class="menu"> <a href="https://www.raymin0223.com/index.html">Home</a>  <a href="https://www.raymin0223.com/index.html#publications">Publications</a>  
	  <a href="https://www.raymin0223.com/index.html#patents"> Patents</a>  <a href="https://www.raymin0223.com/index.html#awards"> Awards</a> 
	  <a href="https://www.raymin0223.com/index.html#experience"> Experience</a> <a href="https://www.raymin0223.com/index.html#projects">Projects</a>   
	  <a href="https://www.raymin0223.com/index.html#services"> Services</a> 
  </div>
  <div class="container">
    <table border="0">
      <tbody><tr>
        <td><img src="./Sangmin_Bae_files/bio-ray.jpg" width="130"></td>
        <td style="width: 10px">&nbsp;</td>
        <td valign="top" width="500">
          <span class="name">Sangmin Bae</span>
          <p class="information"><br>
           Research Scientist, <a href="http://osi.kaist.ac.kr/">OSI Lab</a> </p>
          <p class="information">Graduate School of AI, KAIST<br>
            85 Hoegi-ro, Dongdaemun-gu, Seoul, Korea<br></p>
          <p class="information"><strong>Email</strong>: <span class="unselectable">bsmn0223<span class="mock"></span><span class="hide">xkxkxk</span>@kaist.ac.kr</span> <span class="unselectable">/ bsmn0223<span class="mock"></span><span class="hide">xkxkxk</span>@gmail.com</span> </br>
	  <a href="https://scholar.google.com/citations?user=T5rHY14AAAAJ&hl=en">Google Scholar</a>, <a href="https://drive.google.com/drive/folders/1CBpE2SYBjA9cOm8vblU8gZyugeCkvHvf?usp=share_link">CV</a>, <a href="https://github.com/raymin0223/">Github</a>, <a href="http://www.linkedin.com/in/raymin0223/">Linkedin</a>, <a href="https://twitter.com/raymin0223">X</a> </p>
        </td>
      </tr>
    </tbody></table>
  <strong>Welcome to my page!</strong> I am a Research Scientist with a strong desire to become a  <strong>versatile expert in AI</strong>. I have explored NLP, CV, Audio, Tabular, and Multimodal, to broaden my knowledge and expertise. 
	  <p>My research interests lie in <strong>Efficient AI</strong>, which entails exploring <strong>inference-</strong>, <strong>training-</strong> or <strong>data-efficient</strong> approaches. I'm lately interested in <strong>LLM Inference Acceleration</strong> via <strong>Adaptive Computation</strong> algorithms or various techniques. Moreover, I have experienced in developing <strong>Foundation Models</strong>---pretraining LLMs optimized for accelerated inference, and self-supervised learning for visual models.</p>

   <a id="news" class="anchor"></a><span class="section">News</span>    

    <p class="news">
     <strong>May 2025:</strong>  <img src="./images/meta_logo.png" width="20" height="20"> Starting an internship at Meta FAIR.
    </p>
    <p class="news">
     <strong>May 2025:</strong>  &#x1F38A A paper on 'Audio-Visual Speech Recognition with MoE' accepted at ICML 2025.
    </p>
    <p class="news">
     <strong>Jan. 2025:</strong>  &#x1F38A Three papers on 'Recursive Transformer, Audio-Visual Speech Recognition, Data Selection for Diffusion' accepted at ICLR 2025.
    </p>
    <p class="news">
     <strong>Sep. 2024:</strong>  &#x1F38A A paper on 'Global-to-Local Language Modeling for Fast Inference' accepted at NeurIPS 2024.
    </p>
    <p class="news">
     <strong>May 2024:</strong> <img src="./images/deepmind_logo.png" width="15" height="15"> Starting an internship at Google DeepMind.
    </p>

    <p>&nbsp;</p>

   <a id="education" class="anchor"></a><span class="section">Education</span> 

   <li style="line-height:160%;"> Ph.D. student in Graduate School of AI, KAIST. Advised by Prof. <a href="https://scholar.google.com/citations?user=X_IAjb8AAAAJ&hl=en"><font color="#000080">Se-Young Yun</font></a>. &nbsp;&nbsp;<em>Mar. 2021 - Present</em> </li> 
   <li style="line-height:160%;">  M.S. in Industrial and Systems Engineering, KAIST. Advised by Prof. Se-Young Yun. &nbsp;&nbsp;<em>Mar. 2019 - Feb. 2021</em> </li> 
   <li style="line-height:160%;">  B.S. in Industrial and Systems Engineering. &nbsp;&nbsp;<em>Mar. 2014 - Feb. 2019</em> </li> 
	

   <p>&nbsp;</p>

    <!-- Publication session -->
    <a id="publications" class="anchor"></a><span class="section">Publications <a href="https://scholar.google.com/citations?user=T5rHY14AAAAJ&hl=en">  Google Scholar </a> </span>
    *: 1st co-authors, <sup>&dagger;</sup>: corresponding authors, C: conferences, J: journals, W: workshops, P: preprints </br> </br>
    <table border="0" width="90%" class="paper">
	
	</tbody></table></br>
	<font size="4.5px"><strong>2025</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
		
			
<!--         <tr>
          <td>
            <img src="./images/example-image.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [P6] As a co-author. <strong>Continual Learning for Large Language Models</strong>. <em>Under Review</em> 2024.
          </td>
        </tr>  -->
				
		
        <tr>
          <td>
            <img src="./images/example-image.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [P4] As a first-author. <strong>Dynamic Adaptive Computation</strong>. <em>Under Review</em> 2025.
          </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/example-image.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [P4] As a co-author. <strong>Training-free Guidance for Diffusion</strong>. <em>Under Review</em> 2025.
          </td>
        </tr> 
				
			
        <tr>
          <td>
            <img src="./images/2025_mohave.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C16] Sungnyun Kim, Kangwook Jang, <strong>Sangmin Bae</strong>, Sungwoo Cho, Se-Young Yun<sup>&dagger;</sup>. <strong>MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition</strong>. <em>International Conference on Machine Learning</em> (ICML) 2025.
          	[<a href="https://arxiv.org/abs/2502.10447"><font color="#000080">pdf</font></a>]  
          </td>
        </tr> 
				
		
        <tr>
          <td>
            <img src="./images/2025_recursive.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C15] <strong>Sangmin Bae</strong>, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, Tal Schuster<sup>&dagger;</sup>. <strong>Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</strong>. <em>International Conference on Learning Representations</em> (ICLR) 2025.
          	[<a href="https://arxiv.org/abs/2410.20672"><font color="#000080">pdf</font></a>]  
	  </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/2025_cav2vec.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C14] Sungnyun Kim, Sungwoo Cho, <strong>Sangmin Bae</strong>, Kangwook Jang, Se-Young Yun<sup>&dagger;</sup>. <strong>Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation</strong>.  <em>International Conference on Learning Representations</em> (ICLR) 2025.
	[<a href="https://arxiv.org/abs/2504.18539"><font color="#000080">pdf</font></a>]
	[<a href="https://github.com/sungnyun/cav2vec"><font color="#000080">code</font></a>]
          </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/2025_fifa.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C13] Yongjin Yang*, Sihyeon Kim*, Hojung Jung, <strong>Sangmin Bae</strong>, SangMook Kim, Se-Young Yun<sup>&dagger;</sup>, Kimin Lee<sup>&dagger;</sup>. <strong>Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models</strong>.  <em>International Conference on Learning Representations</em> (ICLR) 2025.
	[<a href="https://arxiv.org/abs/2410.10166"><font color="#000080">pdf</font></a>]  
          </td>
        </tr> 
		
	</tbody></table></br>
	<font size="4.5px"><strong>2024</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
		
        <tr>
          <td>
            <img src="./images/2024_tabforest.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [P2] Felix den Greejen*, <strong>Sangmin Bae</strong>, Stephen Cha, Se-Young Yun<sup>&dagger;</sup>. <strong>Fine-tuned In-Context Learning Transformers are Excellent Tabular Data Classifiers</strong>. <em>Preprint (Under Review)</em> 2024.
            [<a href="https://arxiv.org/abs/2405.13396"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/FelixdenBreejen/TabForestPFN"><font color="#000080">code</font></a>]
	  </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/2024_block.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C12] Namgyu Ho*, <strong>Sangmin Bae</strong>*, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>. <strong>Block Transformer: Global-to-Local Language Modeling for Fast Inference</strong>. <em>Conference on Neural Information Processing Systems</em> (NeurIPS) 2024.
            [<a href="https://arxiv.org/abs/2406.02657"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/itsnamgyu/block-transformer"><font color="#000080">code</font></a>]
	  </td>
        </tr> 

        <tr>
          <td>
            <img src="./images/2024_vacode.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [W7] Sihyeon Kim*, Boryeong Cho*, <strong>Sangmin Bae</strong>, Sumyeong Ahn<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>. <strong>VACoDe: Visual Augmented Contrastive Decoding</strong>. <em>International Conference on Machine Learning Workshop on Trustworthy Multi-modal Foundation Models and AI Agents</em> (ICMLW) <em>Under Review</em> 2024.
            [<a href="https://www.arxiv.org/pdf/2408.05337"><font color="#000080">pdf</font></a>]
	  </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/2024_axa.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C11] Sungnyun Kim*, Kangwook Jang*, <strong>Sangmin Bae</strong>, Hoirin Kim<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>. <strong>Learning Video Temporal Dynamics with Asymmetric Cross-Modal Attention for Robust Audio-Visual Speech Recognition</strong>. <em>IEEE Spoken Language Technology Workshop</em> (SLT) 2024.
            [<a href="https://arxiv.org/pdf/2407.03563v1"><font color="#000080">pdf</font></a>]
	  </td>
        </tr> 
		
        <tr>
          <td>
            <img src="./images/2024_pin.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C10] Yunseon Choi, <strong>Sangmin Bae</strong>, Seonghyun Ban, Minchan Jeong, Chuheng Zhang, Lei Song, Li Zhao, Jiang Bian, Kee-Eung Kim<sup>&dagger;</sup>. <strong>Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL</strong>. <em>The Association for Computational Linguistics</em> (ACL) 2024. <span class="oral">Oral Presentation.</span>
            [<a href="https://www.arxiv.org/abs/2407.14733"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/Youseob/PIN"><font color="#000080">code</font></a>]  
	  </td>
        </tr>  
		
        <tr>
          <td>
            <img src="./images/2024_repaugment.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C9] June-Woo Kim, Miika Toikkanen, <strong>Sangmin Bae</strong>, Minseok Kim<sup>&dagger;</sup>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification</strong>. <em>International Conference of the IEEE Engineering in Medicine and Biology Society</em> (EMBC) 2024.
            [<a href="https://arxiv.org/abs/2405.02996"><font color="#000080">pdf</font></a>]
	  </td>
        </tr>  

	
        <tr>
          <td>
            <img src="./images/2023_carpe_diem.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C8] Yujin Kim, Jaehong Yoon, Seonghyeon Ye, <strong>Sangmin Bae</strong>, Namgyu Ho, Sung Ju Hwang<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.  <strong>Carpe diem: On the Evaluation of World Knowledge in Lifelong Language Models</strong>. <em>Conference of the North American Chapter of the Association for Computational Linguistics</em> (NAACL) Long Paper 2024.
            [<a href="https://arxiv.org/abs/2311.08106"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kimyuji/EvolvingQA_benchmark"><font color="#000080">code</font></a>]  
	  </td>
        </tr>  
		
        <tr>
          <td>
            <img src="./images/2023_sg_scl.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C7] June-Woo Kim, <strong>Sangmin Bae</strong>, Won-Yang Cho, Byungjo Lee, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Stethoscope-guided Supervised Contrastive Learning for Cross-domain Adaptation on Respiratory Sound Classification</strong>. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em> (ICASSP) 2024.
	    [<a href="https://arxiv.org/pdf/2312.09603.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/stethoscope-guided_supervised_contrastive_learning"><font color="#000080">code</font></a>]
          </td>
        </tr>   
	    
	</tbody></table></br>
	<font size="4.5px"><strong>2023</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
		
        <tr>
          <td>
            <img src="./images/2023_adversarial_ft.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [W6] June-Woo Kim, Chihyeon Yoon, Miika Toikkanen, <strong>Sangmin Bae</strong>, Ho-Young Jung<sup>&dagger;</sup>.  <strong>Adversarial Fine-tuning using Generated Respiratory Sound to Address Class Imbalance</strong>. <em>Neural Information Processing Systems Workshop on Deep Generative Models for Health</em> (NeurIPSW) 2023.
            [<a href="https://arxiv.org/pdf/2311.06480.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/kaen2891/adversarial_fine-tuning_using_generated_respiratory_sound"><font color="#000080">code</font></a>]
	  </td>
        </tr>   
	      
        <tr>
          <td>
            <img src="./images/2023_tabular_retrieval.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [W5] Felix den Breejen, <strong>Sangmin Bae</strong>, Stephen Cha, Tae-Young Kim, Seoung-Hyun Koh, Se-Young Yun<sup>&dagger;</sup>.  <strong>Exploring the Retrieval Mechanism for Tabular Deep Learning</strong>. <em>Neural Information Processing Systems Workshop on Table Representation Learning</em> (NeurIPSW) 2023.
            [<a href="https://arxiv.org/pdf/2311.07343.pdf"><font color="#000080">pdf</font></a>]
	  </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_free.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C6] <strong>Sangmin Bae</strong>*, Jongwoo Ko*, Hwanjun Song<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.  <strong>Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding</strong>. <em>Conference on Empirical Methods in Natural Language Processing</em> (EMNLP) Long Paper 2023.            
 	    [<a href="https://arxiv.org/pdf/2310.05424.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/fast_robust_early_exit"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_patchmix.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C5] <strong>Sangmin Bae</strong>*, June-Woo Kim*, Won-Yang Cho, Hyerim Baek, Soyoun Son, Byungjo Lee, Changwan Ha, Kyongpil Tae, Sungnyun Kim<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.  <strong>Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification</strong>. <em>Conference of the International Speech Communication Association </em>  (INTERSPEECH) 2023.
            [<a href="https://arxiv.org/pdf/2305.14032.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/patch-mix_contrastive_learning"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
          <td>
            <img src="./images/2023_openset.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C4] Sungnyun Kim*, <strong>Sangmin Bae</strong>*, Se-Young Yun<sup>&dagger;</sup>.   <strong>Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning</strong>. <em>International Conference on Computer Vision and Pattern Recognition </em>  (CVPR) 2023.
            [<a href="https://arxiv.org/pdf/2303.11101.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/sungnyun/openssl-simcore"><font color="#000080">code</font></a>]
          </td>
        </tr>   

        <tr>
        <td>
            <img src="./images/2023_logo.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td bgcolor="#e9eaed">
            [C3] Sangmook Kim*, <strong>Sangmin Bae</strong>*, Hwanjun Song<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.  <strong>Re-thinking Federated Active Learning based on Inter-class Diversity</strong>. <em>International Conference on Computer Vision and Pattern Recognition </em> (CVPR) 2023. 
            [<a href="https://arxiv.org/pdf/2303.12317.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/LoGo"><font color="#000080">code</font></a>]
          </td>
        </tr>    

        <tr>
        <td>
            <img src="./images/2023_selfcon.png" class="PaperThumbnail" width="120" height="60">
          </td>
          <td>
            [C2] <strong>Sangmin Bae</strong>*, Sungnyun Kim*, Jongwoo Ko, Gihun Lee, Seungjong Noh, Se-Young Yun<sup>&dagger;</sup>.  <strong>Self-Contrastive Learning: Single-viewed Supervised Contrastive Framework using Sub-network</strong>. <em>The Association for the Advancement of Artificial Intelligence </em> (AAAI) 2023. <span class="oral">Oral Presentation.</span>
            [<a href="https://arxiv.org/pdf/2106.15499.pdf"><font color="#000080">pdf</font></a>]
            [<a href="https://github.com/raymin0223/self-contrastive-learning"><font color="#000080">code</font></a>]
          </td>
        </tr>  
	  
	  
	</tbody></table></br>
	<font size="4.5px"><strong>2022</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	   
      <tr>
        <td>
          <img src="./images/2022_fedntd.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
          [C1] Gihun Lee*, Minchan Jeong*, Yongjin Shin, <strong>Sangmin Bae</strong>, Se-Young Yun<sup>&dagger;</sup>. <strong>Preservation of Global Knowledge by Not-True Distillation in Federated Learning</strong>. <em>Neural Information Processing Systems </em> (NeurIPS) 2022. 
		  [<a href="https://arxiv.org/pdf/2106.03097.pdf"><font color="#000080">pdf</font></a>]
		  [<a href="https://github.com/Lee-Gihun/FedNTD"><font color="#000080">code</font></a>]
		  </td>
      </tr>
	  	  

      <tr>
        <td>
          <img src="./images/2022_openset.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
          [W4] Sungnyun Kim*, <strong>Sangmin Bae</strong>*, Se-Young Yun<sup>&dagger;</sup>. <strong>Coreset Sampling from Open-Set for Fine-Grained Self-Supervised Learning</strong>. <em>Neural Information Processing Systems Workshop on Self-Supervised Learning: Theory and Practice</em> (NeurIPSW) 2022. 
		  [<a href="https://sslneurips22.github.io/paper_pdfs/paper_28.pdf"><font color="#000080">pdf</font></a>]
		  </td>
      </tr>

      <tr>
        <td>
          <img src="./images/2022_lg_fal.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
          [W3] Sangmook Kim*, <strong>Sangmin Bae</strong>*, Hwanjun Song<sup>&dagger;</sup>, Se-Young Yun<sup>&dagger;</sup>.  <strong>LG-FAL: Federated Active Learning Strategy using Local and Global Models</strong>. <em>International Conference on Machine Learning Workshop on Adaptive Experimental Design and Active Learning in the Real World </em> (ICMLW) 2022. 
          [<a href="https://realworldml.github.io/files/cr/paper46.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>  
      
	</tbody></table></br>
	<font size="4.5px"><strong>2020</strong></font>
	<table border="0" width="90%" class="paper"><tbody>
	  
        <tr>
        <td>
          <img src="./images/2020_mixco.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
          [W2] Sungnyun Kim*, Gihun Lee*, <strong>Sangmin Bae</strong>*, Se-Young Yun<sup>&dagger;</sup>.  <strong>MixCo: Mix-up Contrastive Learning for Visual Representation</strong>. <em>Neural Information Processing Systems Workshop on Self-Supervised Learning: Theory and Practice </em> (NeurIPSW) 2020. 
          [<a href="https://arxiv.org/pdf/2010.06300.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/Lee-Gihun/MixCo-Mixup-Contrast"><font color="#000080">code</font></a>]
        </td>
      </tr>   

      <tr>
        <td>
          <img src="./images/2020_mabfl.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td bgcolor="#e9eaed">
          [P1] Taehyeon Kim*, <strong>Sangmin Bae</strong>*, Jin-woo Lee, Se-Young Yun<sup>&dagger;</sup>.  <strong>Accurate and Fast Federated Learning via Combinatorial Multi-Armed Bandits</strong>. <em>Preprint </em> 2020. 
          [<a href="https://arxiv.org/pdf/2012.03270.pdf"><font color="#000080">pdf</font></a>]
        </td>
      </tr>   

        <tr>
        <td>
          <img src="./images/2020_sipa.png" class="PaperThumbnail" width="120" height="60">
        </td>
        <td>
          [W1] Gihun Lee*, <strong>Sangmin Bae</strong>*, Jaehoon Oh, Se-Young Yun<sup>&dagger;</sup>.  <strong>SIPA: A Simple Framework for Efficient Networks</strong>. <em>IEEE International Conference on Data Mining Workshop on Big Data Analysis for Smart Engergy </em> (ICDMW) 2020. 
          [<a href="https://arxiv.org/pdf/2004.14476.pdf"><font color="#000080">pdf</font></a>]
          [<a href="https://github.com/Lee-Gihun/MicroNet_OSI-AI"><font color="#000080">code</font></a>]
        </td>
      </tr>   
	  
    </tbody></table>

    <p>&nbsp;</p>
	 
    <a id="patents" class="anchor"></a><span class="section">Patents</span>
    <li style="line-height:130%;"> Se-Young Yun, Seongyoon Kim, Woojin Chung, <strong>Sangmin Bae</strong>. Toward Enhanced Representation for Federated Re-Identification by Not-True Self <br>Knowledge Distillation. Korea Patent Application. &nbsp;&nbsp;<em>Aug. 2022</em></li>
    <li style="line-height:130%;"> Jaehoon Oh, Sangmook Kim, Se-Young Yun, <strong>Sangmin Bae</strong>, Jaewoo Shin, Seongyoon Kim, Woojin Chung. Federated Learning System for Performing Individual Data Customized Federated Learning, Method for Federated Learning, and Client Aratus for Performing Same. Korea and US Patent Application. <br><em>Jun. 2022, Oct. 2022</em></li>
    <li style="line-height:130%;"> Gihun Lee, Minchan Jeong, Se-Young Yun, <strong>Sangmin Bae</strong>, Jaeyeon Ahn, Seongyoon Kim, Woojin Chung. System, Method, Computer-Readable Storage Medium and Computer Program for Federated Learning of Local Model based on Learning Direction of Global Model. Korea and US Patent Application. <br><em>Jun. 2022, Oct. 2022</em></li>	
    
    <p>&nbsp;</p>

	 
    <a id="awards" class="anchor"></a><span class="section">Awards and Honors</span>
    <li style="line-height:160%;">  $3,000 for Google Conference Scholarship Program. &nbsp;&nbsp;<em>Aug. 2025</em> </li>
    <li style="line-height:160%;">  $30,000 in Google Cloud Grants from Google Cloud Platform (GCP). &nbsp;&nbsp;<em>Aug. 2024</em> </li>
    <li style="line-height:160%;">  Silver Award in Signal Processing from Samsung Humantech Paper Awards. &nbsp;&nbsp;<em>Jan. 2024</em> </li>
    <li style="line-height:160%;">  Two Best Presentation Awards from Korea Computing Congress (KCC). &nbsp;&nbsp;<em>Aug. 2022</em> </li>
    <li style="line-height:160%;">  Best Paper Award (5th Place) from Korean AI Association and LG AI Research (JKAIA). &nbsp;&nbsp;<em>Nov. 2021</em></li>
    <li style="line-height:160%;">    <a href="https://micronet-challenge.github.io"><font color="#000080">MicroNet Challenge</font></a> 4th Place at NeurIPS Workshop. &nbsp;&nbsp;<em>Oct. 2019</em> </li>
    <li style="line-height:160%;"> Alumni Scholarship from KAIST. &nbsp;&nbsp;<em>Mar. 2017 - Feb. 2019</em> </li>
    <li style="line-height:160%;"> Dean's List (Top 3%) at Faculty of Engineering Department in KAIST. &nbsp;&nbsp;<em>Spring 2017</em> </li>
    
    <p>&nbsp;</p>

	  
    <a id="experience" class="anchor"></a><span class="section">Research Experience</span>
    <li style="line-height:160%;">  Research Internship at Meta FAIR, advised by <a href="https://scholar.google.com/citations?user=S1szbyAAAAAJ&hl=en"><font color="#000080">Carole-Jean Wu</font></a> and <a href="https://scholar.google.com/citations?user=RMTfWQkAAAAJ&hl=en"><font color="#000080">Bilge Acun</font></a>. &nbsp;&nbsp;<em>From May 2025 - Present</em> </li>
    <li style="line-height:160%;">  Research Collaboration with Google, advised by <a href="https://scholar.google.co.il/citations?user=oo8QRmIAAAAJ&hl=en"><font color="#000080">Tal Schuster</font></a>, <a href="https://scholar.google.com/citations?user=LYRkQhMAAAAJ&hl=en"><font color="#000080">Adam Fisch</font></a>,  <a href="https://scholar.google.com/citations?user=XKbglPEAAAAJ&hl=en"><font color="#000080">Seungyeon Kim</font></a>,  <a href="https://scholar.google.com/citations?user=3l_6H5sAAAAJ&hl=en"><font color="#000080">Ziwei Ji</font></a>, <a href="https://scholar.google.com/citations?user=GaCGz8wAAAAJ&hl=en"><font color="#000080">Hrayr Harutyunyan</font></a>. &nbsp;&nbsp;<em>Aug. 2024 - Nov. 2024</em> </li>
    <li style="line-height:160%;">  Research Internship at Google DeepMind, advised by <a href="https://scholar.google.co.il/citations?user=oo8QRmIAAAAJ&hl=en"><font color="#000080">Tal Schuster</font></a> and <a href="https://scholar.google.com/citations?user=LYRkQhMAAAAJ&hl=en"><font color="#000080">Adam Fisch</font></a>. &nbsp;&nbsp;<em>May 2024 - Aug. 2024</em> </li>
    <li style="line-height:160%;">  Research Collaboration with MODULABS. &nbsp;&nbsp;<em>Sep. 2022 - Jan. 2024</em> </li>
    <li style="line-height:160%;">  Research Collaboration with NAVER AI, advised by <a href="https://scholar.google.com/citations?user=Ijzuc-8AAAAJ&hl=en"><font color="#000080">Hwanjun Song</font></a>. &nbsp;&nbsp;<em>Jan. 2022 - Jan. 2023</em> </li>
    <li style="line-height:160%;">  Research Internship at Kakao Recommendation Team. &nbsp;&nbsp;<em>Sep. 2018 - Feb. 2019</em> </li>
    <li style="line-height:160%;">   Research Internship at Optimization and Statistical Inference Lab, KAIST. &nbsp;&nbsp;<em>Jul. 2018 - Aug. 2018</em> </li>
    <li style="line-height:160%;">  Research Internship at Human Factors and Ergonomics Lab, KAIST. &nbsp;&nbsp;<em>Dec. 2017 - Jun. 2018</em> </li>
    <li style="line-height:160%;">   Exchange Student at  Linköping University. &nbsp;&nbsp;<em>Jul. 2017 - Aug. 2017</em>    </li>

    <p>&nbsp;</p>

	 
    <a id="projects" class="anchor"></a><span class="section">Research Projects</span>
    
    <li style="line-height:160%;"> [<strong>IITP</strong>] National AI Research Lab.  <span class="oral">Project Manager</span>. &nbsp;&nbsp;<em>Nov. 2024 - Present</em>  </li> 
    <li style="line-height:160%;"> [<strong>KT</strong>] Efficient large language model inference algorithm. &nbsp;&nbsp;<em>Sep. 2024 - Oct. 2024</em>  </li> 
    <li style="line-height:160%;"> [<strong>NIER</strong>] Short-term Prediction of Particulate Matter via Artificial Intelligence. <span class="oral">Project Manager</span>. &nbsp;&nbsp;<em>Mar. 2023 - May. 2024</em>  </li> 
    <li style="line-height:160%;">  [<strong>KT</strong>] Neural Architecture Search for Detecting Communication Network Failure. <span class="oral">Project Manager</span>. &nbsp;&nbsp;<em>Apr. 2022 - Feb. 2023</em>   </li> 
    <li style="line-height:160%;">  [<strong>ETRI</strong>] Lightweight Edge Device Technology via Federated Learning. <span class="oral">Project Manager</span>. &nbsp;&nbsp;<em>Mar. 2021 - Sep. 2022</em>  </li> 
    <li style="line-height:160%;"> [<strong>SK Hynix</strong>] Semantic Segmentation to Detect Errors in Wafer Process. &nbsp;&nbsp;<em>Feb. 2021 - Sep. 2021</em>  </li>  
    <li style="line-height:160%;"> [<strong>ETRI</strong>] Data-efficient Unsupervised Representation Learning. &nbsp;&nbsp;<em>Mar. 2020 - Dec. 2020</em>  </li> 
    <li style="line-height:160%;"> [<strong>ETRI</strong>] Model Compression for Big Data Ddge Analysis. &nbsp;&nbsp;<em>Jun. 2019 - Oct. 2019</em>  </li>
    <li style="line-height:160%;"> [<strong>Hankook Tire and Technology</strong>] Compound Prediction with Artificial Intelligence and Auto-ML. &nbsp;&nbsp;<em>Mar. 2019 - Feb. 2020</em>  </li>

    <p>&nbsp;</p>
	 
    <a id="services" class="anchor"></a><span class="section">Services</span>
    <li style="line-height:160%;">  Server Manager at KAIST AI. &nbsp;&nbsp;<em>Mar. 2021 - Feb. 2023</em> </li>
    <li style="line-height:160%;">  Student Leader at OSI Lab, KAIST. &nbsp;&nbsp;<em>Mar. 2021 - Mar. 2022</em></li>
    <li style="line-height:160%;">  Teaching Assistant. </li>
	<ul>
		<li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; KAIST AI505 Optimization for AI. &nbsp;&nbsp;<em>Fall 2021, Fall 2022, Fall 2023</em> </li>
		<li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; KAIST AI603 Machine Learning Theory . &nbsp;&nbsp;<em>Spring 2021, Spring 2023</em> </li>
		<li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; KAIST AI611 Deep Reinforcement Learning. &nbsp;&nbsp;<em>Spring 2022</em> </li>
	</ul>
    <li style="line-height:160%;"> Instructor on DL and ML courses. </li>
    <ul>
	    <li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; MetaCode: Machine Learning Course. [<a href="https://www.youtube.com/watch?v=oyzIT1g1Z3U&t=0s"><font color="#000080">video</font></a>] (views 120K) &nbsp;&nbsp;<em>Jun. 2021 - Dec. 2022</em> </li>
	    <li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; ForumM: Recommendation Seminar. &nbsp;&nbsp;<em>Nov. 2022</em> </li>
	    <li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; LG-KAIST AI: Computer Vision Course. &nbsp;&nbsp;<em>Oct. 2020, Oct. 2021</em> </li>
	    <li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; Korea Blockchain Institute: Machine Learning Course. &nbsp;&nbsp;<em>Dec. 2020</em> </li>
	    <li style="line-height:160%; margin-left: 10px; list-style-type: none;">&minus; Samsung DS: Deep Learning Course. &nbsp;&nbsp;<em>Jul. 2020</em> </li>
    </ul>
    
    </br>  

  <p><font color="#444444" face="Arial" size="2">&copy 2023 Sangmin Bae Thanks <a href="https://songhwanjun.github.io"><font color="#000080">Dr. Hwanjun Song</font></a> for the template. </font></p>

  </div>
  <script>
    var thumbnails = document.getElementsByClassName("PaperThumbnail");
    var i;
    for (i = 0; i < thumbnails.length; i++) {
      thumbnails[i].width = "120"
    }
  </script>  


</body></html>
